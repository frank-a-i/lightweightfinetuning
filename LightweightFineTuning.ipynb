{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "Using a foundation model that reads a given review text and makes a meaning of the given prompt, making it suitable for estimating a numeric score related to that review text\n",
    "\n",
    "* __Fine-tuning dataset__: [yelp_review_full](https://huggingface.co/datasets/Yelp/yelp_review_full) containing review texts and corresponding 5 star ratings.\n",
    "* __Model__: [DistilBERT base model - uncased](https://huggingface.co/distilbert/distilbert-base-uncased) a model that processes a prompt and estimates appropriate next words\n",
    "* __Evaluation approach__: Since the model should estimate the star rating that can range from 1..5, common classification metrics are used: mainly cross entropy and optional accuracy, precision.\n",
    "* __PEFT technique__: Fine tuning the full model, only the trailing layer and LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "First load the foundation model, tokenizer and the dataset from huggingface.co. Since this is a relatively huge dataset, only a fraction of its content is used to demonstrate the techniques. The share is adjustable in parameter `dataset_size` that equally affects training and testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Loading model + dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] size of: training set=6500 test set=6500\n",
      "[info] Labels to estimate:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: '1 star', 1: '2 star', 2: '3 stars', 3: '4 stars', 4: '5 stars'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from peft import LoraConfig, get_peft_model, AutoPeftModelForSequenceClassification\n",
    "from sklearn.metrics import precision_score, accuracy_score\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from datasets import load_dataset\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(\"[info] Loading model + dataset\")\n",
    "dataset_size = \"1%\"\n",
    "raw_dataset = {\"train\": None, \"test\": None}\n",
    "raw_dataset[\"train\"] = load_dataset(\"Yelp/yelp_review_full\", split=f\"train[:{dataset_size}]\")\n",
    "raw_dataset[\"test\"] = load_dataset(\"Yelp/yelp_review_full\", split=f\"test[:{dataset_size}]\")\n",
    "\n",
    "labels = raw_dataset[\"train\"].features[\"label\"].names\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels=len(labels))\n",
    "\n",
    "\n",
    "print(f\"[info] size of: training set={len(raw_dataset['train'])} test set={len(raw_dataset['train'])}\")\n",
    "print(\"[info] Labels to estimate:\")\n",
    "classnum_to_label = {i: l for i, l in enumerate(labels)}\n",
    "classnum_to_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003e46ea",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Next turn the still human readable review text into a dataset of a format that becomes processible by the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Tokenize dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 6500\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"[info] Tokenize dataset\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenized_ds = dict()\n",
    "for split in raw_dataset.keys():\n",
    "    tokenized_ds[split] = raw_dataset[split].map(\n",
    "        lambda x: tokenizer(x[\"text\"], truncation=True, padding=\"max_length\")\n",
    "    )\n",
    "\n",
    "tokenized_ds[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a1e99",
   "metadata": {},
   "source": [
    "### A peek into the data\n",
    "Now that all is prepared, let's have a look what we're working with. The predicition stems from the vanilla DistilBERT model, adjusted to only output a star rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have been to this restaurant twice and was disappointed both times. I won't go back. The first time we were there almost 3 hours. It took forever to order and then forever for our food to come and the place was empty. When I complained the manager was very rude and tried to blame us for taking to long to order. It made no sense, how could we order when the waitress wasn't coming to the table? After arguing with me he ended up taking $6 off of our $200+ bill. Ridiculous. If it were up to me I would have never returned. Unfortunately my family decided to go here again tonight. Again it took a long time to get our food. My food was cold and bland, my kids food was cold. My husbands salmon was burnt to a crisp and my sister in law took one bite of her trout and refused to eat any more because she claims it was so disgusting. The wedding soup and bread were good, but that's it! My drink sat empty throughout my meal and never got refilled even when I asked. Bad food, slow service and rude managers. I'll pass on this place if my family decides to go again. Not worth it at all with all the other good Italian options around.\n",
      "\n",
      "BERT prediction: 1 star GT: 1 star\n"
     ]
    }
   ],
   "source": [
    "test_line = tokenizer(tokenized_ds[\"test\"][\"text\"][3], truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(**test_line).logits\n",
    "    \n",
    "print(f'{raw_dataset[\"test\"][3][\"text\"]}\\n\\nBERT prediction: {classnum_to_label[torch.argmax(pred).item()]} GT: {classnum_to_label[raw_dataset[\"test\"][3][\"label\"]]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a36f7f7",
   "metadata": {},
   "source": [
    "### Common function\n",
    "In the following functions are defined that are being used a couple of times for retraining & evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d7e551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    centropy = torch.nn.functional.cross_entropy(torch.tensor(logits), torch.tensor(labels))\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions), \n",
    "            \"precision\": precision_score(labels, predictions, average='micro'), \n",
    "            \"eval_cross_entropy\": centropy.item()}\n",
    "\n",
    "def fine_tuning_pipeline(model, store_dir, no_training=False):\n",
    "    \n",
    "    trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=f\"/tmp/genai/lighweightfinetuning/{store_dir}\",\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"cross_entropy\",\n",
    "        label_names=[\"labels\"],\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"test\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    print(\"[info] running evaluation\")\n",
    "    with torch.no_grad():\n",
    "        pre_results = trainer.evaluate()  \n",
    "    if no_training:\n",
    "        return {\n",
    "            \"accuracy\": pre_results[\"eval_accuracy\"], \n",
    "            \"precision\": pre_results[\"eval_precision\"]\n",
    "        }\n",
    "    else:  \n",
    "        print(\"[info] retraining\")\n",
    "        trainer.train()\n",
    "        print(\"[info] evaluating retraining\")\n",
    "        post_results = trainer.evaluate()\n",
    "        model.save_pretrained(f\"{trainer.args.output_dir}/exported_model\", from_pt=True)\n",
    "\n",
    "        return {\n",
    "            \"pre training accuracy\": pre_results[\"eval_accuracy\"], \n",
    "            \"post training accuracy\": post_results[\"eval_precision\"],\n",
    "            \"pre training precision\": pre_results[\"eval_accuracy\"],\n",
    "            \"post training precision\": post_results[\"eval_precision\"]\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "As mentioned in the beginning, sequentially a full retraining, a training limited to only one layer and a LoRA optimization will be conducted. This is to refelect the impact of the three strategies.\n",
    "\n",
    "### Retrain the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_90319/4177661193.py:11: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] running evaluation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 5:18:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] retraining\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1016' max='1016' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1016/1016 5:13:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cross Entropy</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.608300</td>\n",
       "      <td>1.607545</td>\n",
       "      <td>1.607545</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.212800</td>\n",
       "      <td>0.212800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] evaluating retraining\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 04:57]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pre training accuracy': 0.2624, 'post training accuracy': 0.2128, 'pre training precision': 0.2624, 'post training precision': 0.2128}\n"
     ]
    }
   ],
   "source": [
    "model_full_training = deepcopy(model)\n",
    "for param in model_full_training.parameters():\n",
    "    param.requires_grad = True\n",
    "full_training_result = fine_tuning_pipeline(model_full_training, \"full\")\n",
    "print(full_training_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113d454f",
   "metadata": {},
   "source": [
    "### Trailing layer training\n",
    "\n",
    "Here we're freezing the whole model except of the last one, so let's have a look how the model looks like and which layer should be picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34af7090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_one_layer_training = deepcopy(model)\n",
    "model_one_layer_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e99f04",
   "metadata": {},
   "source": [
    "Seems we're unfreezing layer `classifier` then, the one that has been resized to the number of star-rating to estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] running evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_90319/4177661193.py:11: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 2:17:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] retraining\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1016' max='1016' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1016/1016 2:12:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cross Entropy</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.335900</td>\n",
       "      <td>1.308855</td>\n",
       "      <td>1.308855</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.467600</td>\n",
       "      <td>0.467600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] evaluating retraining\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 05:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pre training accuracy': 0.2624, 'post training accuracy': 0.4676, 'pre training precision': 0.2624, 'post training precision': 0.4676}\n"
     ]
    }
   ],
   "source": [
    "for param in model_one_layer_training.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_one_layer_training.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "one_layer_training_result = fine_tuning_pipeline(model_one_layer_training, \"streamlined\")\n",
    "print(one_layer_training_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0811cb",
   "metadata": {},
   "source": [
    "### LoRA Training\n",
    "Similar to above, the trailing classification part is targeted, that makes it a little fairer for comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3118/1620563657.py:11: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] running evaluation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 03:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] retraining\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 02:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cross Entropy</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.609801</td>\n",
       "      <td>1.609801</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.222000</td>\n",
       "      <td>0.222000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] evaluating retraining\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5/16 00:14 < 00:40, 0.27 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_lora_training = deepcopy(model)\n",
    "PEFT_cfg = LoraConfig(\n",
    "    target_modules=\"classifier\",\n",
    ")\n",
    "peft_model = get_peft_model(model=model_lora_training, peft_config=PEFT_cfg)\n",
    "lora_training_result = fine_tuning_pipeline(peft_model, \"lora\")\n",
    "print(lora_training_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "Since every model has been saved persistently, it should be possible to load those exports and check if the evaluation results still match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "modules_to_save cannot be applied to modules of type <class 'peft.tuners.lora.layer.Linear'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoPeftModelForSequenceClassification\n\u001b[0;32m----> 2\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoPeftModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tmp/genai/lighweightfinetuning/lora/exported_model/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPEFT_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m results \u001b[38;5;241m=\u001b[39m fine_tuning_pipeline(loaded_model, store_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp\u001b[39m\u001b[38;5;124m\"\u001b[39m, no_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "File \u001b[0;32m~/.venvs/genai/lib/python3.11/site-packages/peft/auto.py:130\u001b[0m, in \u001b[0;36m_BaseAutoPeftModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, adapter_name, is_trainable, config, revision, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    126\u001b[0m         pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    128\u001b[0m     base_model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(tokenizer))\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target_peft_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_trainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/genai/lib/python3.11/site-packages/peft/peft_model.py:570\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf model_id is a local path, then `adapters` must be passed in kwargs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mtask_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 570\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    578\u001b[0m     model \u001b[38;5;241m=\u001b[39m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config\u001b[38;5;241m.\u001b[39mtask_type](\n\u001b[1;32m    579\u001b[0m         model,\n\u001b[1;32m    580\u001b[0m         config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    583\u001b[0m         low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage,\n\u001b[1;32m    584\u001b[0m     )\n",
      "File \u001b[0;32m~/.venvs/genai/lib/python3.11/site-packages/peft/peft_model.py:1398\u001b[0m, in \u001b[0;36mPeftModelForSequenceClassification.__init__\u001b[0;34m(self, model, peft_config, adapter_name, **kwargs)\u001b[0m\n\u001b[1;32m   1395\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;66;03m# to make sure classifier layer is trainable; this may add a new ModulesToSaveWrapper\u001b[39;00m\n\u001b[0;32m-> 1398\u001b[0m \u001b[43m_set_trainable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/genai/lib/python3.11/site-packages/peft/utils/other.py:403\u001b[0m, in \u001b[0;36m_set_trainable\u001b[0;34m(model, adapter_name)\u001b[0m\n\u001b[1;32m    401\u001b[0m     target\u001b[38;5;241m.\u001b[39mset_adapter(target\u001b[38;5;241m.\u001b[39mactive_adapter)\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m \u001b[43mModulesToSaveWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m     new_module\u001b[38;5;241m.\u001b[39mset_adapter(adapter_name)\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(parent, target_name, new_module)\n",
      "File \u001b[0;32m~/.venvs/genai/lib/python3.11/site-packages/peft/utils/other.py:200\u001b[0m, in \u001b[0;36mModulesToSaveWrapper.__init__\u001b[0;34m(self, module_to_save, adapter_name)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_adapters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(adapter_name)\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/genai/lib/python3.11/site-packages/peft/utils/other.py:218\u001b[0m, in \u001b[0;36mModulesToSaveWrapper.check_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_module, BaseTunerLayer):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# e.g. applying modules_to_save to a lora layer makes no sense\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     cls_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodules_to_save cannot be applied to modules of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: modules_to_save cannot be applied to modules of type <class 'peft.tuners.lora.layer.Linear'>"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "loaded_model = AutoPeftModelForSequenceClassification.from_pretrained(\"/tmp/genai/lighweightfinetuning/lora/exported_model/\", config=PEFT_cfg)\n",
    "results = fine_tuning_pipeline(loaded_model, store_dir=\"/tmp\", no_training=True)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74adfde1",
   "metadata": {},
   "source": [
    "Let's check again with the statistics from directly after the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "796e2464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pre training accuracy': 0.2624, 'post training accuracy': 0.474, 'pre training precision': 0.2624, 'post training precision': 0.474}\n"
     ]
    }
   ],
   "source": [
    "print(lora_training_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476064dc",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "When retraining the full model the performance droped, it is likely to be related with `catastropihc forgetting`. In the opposite just retraining one single layer improved the data comprehension compared to its vanilla state. The results are similar with the LoRA method, where the same layer was targeted but it even slightly outperformed.\n",
    "\n",
    "Eventually storing, loading and reevaluating once again showed the same results, showing a successfull persistent ex- and import of the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
